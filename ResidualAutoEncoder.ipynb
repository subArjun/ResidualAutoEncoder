{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from ResidualAutoEncoder import ResidualAutoencoder, wandb_sweep\n",
    "from ResidualAutoEncoder import Criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Arjun\\anaconda3\\envs\\PyTorch\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Arjun\\anaconda3\\envs\\PyTorch\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "int is not an Optimizer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 51\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tensor \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.5\u001b[39m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 51\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 30\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     27\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_harness\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels/residual_autoencoder.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Arjun\\ResidualAutoEncoder\\ResidualAutoEncoder.py:187\u001b[0m, in \u001b[0;36mResidualAutoencoder.train_harness\u001b[1;34m(self, model, train_loader, test_loader, criterion, optimizer, epochs)\u001b[0m\n\u001b[0;32m    185\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m    186\u001b[0m scaler \u001b[38;5;241m=\u001b[39m GradScaler()\n\u001b[1;32m--> 187\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m \u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlr_scheduler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mReduceLROnPlateau\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m    189\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Arjun\\anaconda3\\envs\\PyTorch\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:1024\u001b[0m, in \u001b[0;36mReduceLROnPlateau.__init__\u001b[1;34m(self, optimizer, mode, factor, patience, threshold, threshold_mode, cooldown, min_lr, eps, verbose)\u001b[0m\n\u001b[0;32m   1022\u001b[0m \u001b[38;5;66;03m# Attach optimizer\u001b[39;00m\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(optimizer, Optimizer):\n\u001b[1;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(optimizer)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not an Optimizer\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m   1025\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;241m=\u001b[39m optimizer\n\u001b[0;32m   1027\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(min_lr, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n",
      "\u001b[1;31mTypeError\u001b[0m: int is not an Optimizer"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Hyperparameters\n",
    "    batch_size = 16\n",
    "    learning_rate = 0.001\n",
    "    epochs = 15\n",
    "    \n",
    "    # Device configuration\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Data transformation and loading\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5)),\n",
    "        transforms.Resize((128, 128))\n",
    "    ])\n",
    "    train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "    test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=3, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "    \n",
    "\n",
    "    \n",
    "    # Model, loss function, and optimizer\n",
    "    model = ResidualAutoencoder(num_blocks=3, block_depth=2, bottleneck_dim=128, channels=3, device=device)\n",
    "    criterion = Criterion(model=model)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Train the model\n",
    "    model.train_harness(model, train_loader, criterion, optimizer, epochs)\n",
    "    \n",
    "    torch.save(model.state_dict(), 'models/residual_autoencoder.pth')\n",
    "    # Evaluate the model\n",
    "    inputs, outputs = model.evaluate_harness(model, test_loader, device)\n",
    "    outputs = unnormalize(outputs)\n",
    "    # Visualize the results\n",
    "    num_images = 10\n",
    "    fig, axes = plt.subplots(2, num_images, figsize=(15, 4))\n",
    "    for i in range(num_images):\n",
    "        axes[0, i].imshow(inputs[i].permute(1, 2, 0))\n",
    "        axes[0, i].axis('off')\n",
    "        axes[1, i].imshow(outputs[i].permute(1, 2, 0))\n",
    "        axes[1, i].axis('off')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "def unnormalize(tensor):\n",
    "    return tensor * 0.5 + 0.5\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvinmed-arjun\u001b[0m (\u001b[33msubarjun\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: z9gikt4e\n",
      "Sweep URL: https://wandb.ai/subarjun/residual-autoencoder/sweeps/z9gikt4e\n"
     ]
    }
   ],
   "source": [
    "sweep_config = {\n",
    "    'method': 'random',  # or 'grid' or 'bayes'\n",
    "    'metric': {\n",
    "        'name': 'val_loss',\n",
    "        'goal': 'minimize'\n",
    "    },\n",
    "    'parameters': {\n",
    "        'num_blocks': {\n",
    "            'values': [2, 3, 4]\n",
    "        },\n",
    "        'block_depth': {\n",
    "            'values': [2, 3]\n",
    "        },\n",
    "        'bottleneck_dim': {\n",
    "            'values': [64, 128, 256]\n",
    "        },\n",
    "        'asym_block': {\n",
    "            'values': [1]\n",
    "        },\n",
    "        'asym_depth': {\n",
    "            'values': [1]\n",
    "        },\n",
    "        'learning_rate': {\n",
    "        'distribution': 'q_log_uniform_values',\n",
    "        'q' : 1e-6,\n",
    "        'max': 0.01,\n",
    "        'min': 1e-6,\n",
    "        },\n",
    "        'l1_lambda': {\n",
    "        'distribution': 'q_log_uniform_values',\n",
    "        'q' : 1e-15,\n",
    "        'max': 1e-6,\n",
    "        'min': 1e-15,\n",
    "        },\n",
    "        'l1_lambda_bottleneck': {\n",
    "        'distribution': 'q_log_uniform_values',\n",
    "        'q' : 1e-10,\n",
    "        'max': 1e-4,\n",
    "        'min': 1e-10,\n",
    "        },\n",
    "        'lambda_kl': {\n",
    "        'distribution': 'q_log_uniform_values',\n",
    "        'q' : 1e-10,\n",
    "        'max': 1e-4,\n",
    "        'min': 1e-10,\n",
    "        },\n",
    "        'lambda_tv': {\n",
    "        'distribution': 'q_log_uniform_values',\n",
    "        'q' : 1e-19,\n",
    "        'max': 1e-8,\n",
    "        'min': 1e-19,\n",
    "        },\n",
    "        'lambda_perceptual': {\n",
    "        'distribution': 'q_log_uniform_values',\n",
    "        'q' : 1e-6,\n",
    "        'max': 1e-1,\n",
    "        'min': 1e-6,\n",
    "        },\n",
    "        'lambda_mse': {\n",
    "        'distribution': 'q_log_uniform_values',\n",
    "        'q' : 1e1,\n",
    "        'min': 1e1,\n",
    "        'max': 1e2,\n",
    "        },\n",
    "        'lambda_ssim': {\n",
    "        'distribution': 'q_log_uniform_values',\n",
    "        'q' : 1e-6,\n",
    "        'max': 1e-1,\n",
    "        'min': 1e-6,\n",
    "        },\n",
    "        'lambda_psnr': {\n",
    "        'distribution': 'q_log_uniform_values',\n",
    "        'q' : 1e-19,\n",
    "        'max': 1e-8,\n",
    "        'min': 1e-19,\n",
    "        },\n",
    "        'dropout': {\n",
    "            'values': [0.2, 0.3, 0.4]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"residual-autoencoder\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: kr9dgixv with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tasym_block: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tasym_depth: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tblock_depth: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbottleneck_dim: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tl1_lambda: 2.497098e-09\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tl1_lambda_bottleneck: 1.91e-08\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlambda_kl: 3.5465000000000003e-06\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlambda_mse: 130\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlambda_perceptual: 4.2e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlambda_psnr: 8.4803911665e-09\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlambda_ssim: 0.0008849999999999999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlambda_tv: 2.1398006735e-09\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001109\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_blocks: 4\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Arjun\\ResidualAutoEncoder\\wandb\\run-20240530_011422-kr9dgixv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/subarjun/residual-autoencoder/runs/kr9dgixv' target=\"_blank\">fast-sweep-1</a></strong> to <a href='https://wandb.ai/subarjun/residual-autoencoder' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/subarjun/residual-autoencoder/sweeps/z9gikt4e' target=\"_blank\">https://wandb.ai/subarjun/residual-autoencoder/sweeps/z9gikt4e</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/subarjun/residual-autoencoder' target=\"_blank\">https://wandb.ai/subarjun/residual-autoencoder</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/subarjun/residual-autoencoder/sweeps/z9gikt4e' target=\"_blank\">https://wandb.ai/subarjun/residual-autoencoder/sweeps/z9gikt4e</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/subarjun/residual-autoencoder/runs/kr9dgixv' target=\"_blank\">https://wandb.ai/subarjun/residual-autoencoder/runs/kr9dgixv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Arjun\\anaconda3\\envs\\PyTorch\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Arjun\\anaconda3\\envs\\PyTorch\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "c:\\Users\\Arjun\\anaconda3\\envs\\PyTorch\\Lib\\site-packages\\torch\\autograd\\graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cudnn\\Conv_v8.cpp:919.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/25], Loss: 93.9509, recon_loss: 0.0305, val_loss: 1432.1378 learning rate: 0.001109\n",
      "Epoch [2/25], Loss: 91.5569, recon_loss: 0.0172, val_loss: 917.6717 learning rate: 0.001109\n",
      "Epoch [3/25], Loss: 91.3299, recon_loss: 0.0190, val_loss: 860.1141 learning rate: 0.001109\n",
      "Epoch [4/25], Loss: 91.1935, recon_loss: 0.0186, val_loss: 784.1443 learning rate: 0.001109\n",
      "Epoch [5/25], Loss: 91.1371, recon_loss: 0.0153, val_loss: 844.8107 learning rate: 0.001109\n",
      "Epoch [6/25], Loss: 91.1039, recon_loss: 0.0141, val_loss: 709.7896 learning rate: 0.001109\n",
      "Epoch [7/25], Loss: 91.0191, recon_loss: 0.0114, val_loss: 683.7635 learning rate: 0.001109\n",
      "Epoch [8/25], Loss: 91.0060, recon_loss: 0.0166, val_loss: 672.7143 learning rate: 0.001109\n",
      "Epoch [9/25], Loss: 91.0112, recon_loss: 0.0143, val_loss: 670.0866 learning rate: 0.001109\n"
     ]
    }
   ],
   "source": [
    "wandb.agent(sweep_id, wandb_sweep, count=25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
